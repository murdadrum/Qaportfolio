<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>AI/LLM Quality Assurance Platform</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600&display=swap"
      rel="stylesheet"
    />
    <style>
      :root {
        color-scheme: dark;
        --bg-900: #0b1220;
        --bg-800: #0f1b2d;
        --bg-700: #14243b;
        --text-100: #e2e8f0;
        --text-300: #cbd5f5;
        --text-500: #94a3b8;
        --accent-500: #38bdf8;
        --accent-400: #7dd3fc;
        --accent-300: #a5b4fc;
        --success-400: #4ade80;
      }
      * {
        box-sizing: border-box;
      }
      body {
        margin: 0;
        font-family: "Space Grotesk", "IBM Plex Sans", system-ui, sans-serif;
        background: radial-gradient(
          circle at top,
          #1d2b4a 0%,
          #0b1220 48%,
          #020617 100%
        );
        color: var(--text-100);
        min-height: 100vh;
      }
      a {
        color: var(--accent-400);
        text-decoration: none;
      }
      a:hover {
        color: var(--accent-500);
      }
      .page {
        max-width: 1120px;
        margin: 0 auto;
        padding: 48px 20px 72px;
      }
      .top-bar {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 24px;
      }
      .badge {
        display: inline-flex;
        align-items: center;
        gap: 8px;
        padding: 6px 14px;
        border-radius: 999px;
        background: rgba(59, 130, 246, 0.18);
        color: #c7d2fe;
        font-size: 12px;
        text-transform: uppercase;
        letter-spacing: 0.18em;
      }
      .cta {
        display: inline-flex;
        align-items: center;
        gap: 8px;
        padding: 8px 16px;
        border-radius: 999px;
        background: rgba(56, 189, 248, 0.16);
        color: #bae6fd;
        font-size: 13px;
      }
      header {
        display: grid;
        gap: 20px;
        margin-bottom: 36px;
      }
      .title {
        font-size: clamp(32px, 4vw, 44px);
        line-height: 1.05;
        margin: 0;
      }
      .subtitle {
        font-size: 18px;
        color: var(--text-500);
        max-width: 760px;
        margin: 0;
      }
      .meta {
        display: flex;
        flex-wrap: wrap;
        gap: 12px;
        align-items: center;
      }
      .pill {
        display: inline-flex;
        align-items: center;
        padding: 6px 12px;
        border-radius: 999px;
        background: rgba(148, 163, 184, 0.16);
        font-size: 12px;
        color: var(--text-100);
      }
      .section {
        margin-top: 32px;
      }
      .section h2 {
        font-size: 16px;
        text-transform: uppercase;
        letter-spacing: 0.2em;
        color: var(--accent-300);
        margin: 0 0 14px;
      }
      .grid {
        display: grid;
        gap: 16px;
        grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
      }
      .card {
        padding: 18px;
        border-radius: 16px;
        background: rgba(15, 23, 42, 0.7);
        border: 1px solid rgba(148, 163, 184, 0.12);
        box-shadow: 0 18px 40px rgba(2, 6, 23, 0.45);
      }
      .card h3 {
        margin: 0 0 8px;
        font-size: 16px;
      }
      .card p {
        margin: 0;
        color: var(--text-500);
        font-size: 14px;
        line-height: 1.5;
      }
      .list {
        display: grid;
        gap: 10px;
        margin: 0;
        padding: 0;
      }
      .list-item {
        display: flex;
        gap: 10px;
        align-items: flex-start;
        font-size: 14px;
        color: var(--text-300);
      }
      .dot {
        width: 8px;
        height: 8px;
        border-radius: 50%;
        background: linear-gradient(
          135deg,
          var(--accent-500),
          var(--accent-300)
        );
        margin-top: 6px;
        flex-shrink: 0;
      }
      .kpi {
        display: grid;
        gap: 6px;
        border-radius: 14px;
        padding: 14px;
        background: rgba(20, 36, 59, 0.6);
      }
      .kpi span {
        color: var(--text-500);
        font-size: 12px;
        text-transform: uppercase;
        letter-spacing: 0.14em;
      }
      .kpi strong {
        font-size: 16px;
        color: var(--text-100);
      }
      .pipeline {
        display: grid;
        gap: 12px;
      }
      .pipeline-step {
        display: grid;
        gap: 6px;
        padding: 14px 16px;
        border-radius: 14px;
        background: rgba(34, 197, 94, 0.12);
        color: #dcfce7;
      }
      .pipeline-step span {
        font-size: 12px;
        letter-spacing: 0.12em;
        text-transform: uppercase;
        color: #bbf7d0;
      }
      .code-block {
        background: #0b1325;
        border: 1px solid rgba(148, 163, 184, 0.2);
        border-radius: 16px;
        padding: 16px;
        font-family: "SF Mono", "JetBrains Mono", monospace;
        font-size: 12px;
        color: #e2e8f0;
        white-space: pre-wrap;
      }
      .flow {
        display: flex;
        flex-wrap: wrap;
        gap: 10px;
      }
      .flow span {
        padding: 6px 12px;
        border-radius: 12px;
        background: rgba(34, 197, 94, 0.14);
        color: #bbf7d0;
        font-size: 12px;
      }
      footer {
        margin-top: 40px;
        color: var(--text-500);
        font-size: 12px;
      }
      @media (max-width: 700px) {
        .top-bar {
          flex-direction: column;
          align-items: flex-start;
          gap: 12px;
        }
      }
    </style>
  </head>
  <body>
    <div class="page">
      <div class="top-bar">
        <span class="badge">QA Case Study</span>
        <a class="cta" href="/">‚Üê Back to Portfolio</a>
      </div>

      <header>
        <h1 class="title">AI/LLM Quality Assurance Platform</h1>
        <p class="subtitle">
          Testing framework for LLM applications with prompt evaluation, safety
          checks, and regression tracking.
        </p>
        <div class="meta">
          <span class="pill">Python</span><span class="pill">OpenAI API</span
          ><span class="pill">Pytest</span
          ><span class="pill">Machine Learning</span
          ><span class="pill">Data Analytics</span>
          <a
            class="pill"
            href="https://github.com/murdadrum/ai-llm-quality-assurance-platform"
            target="_blank"
            rel="noreferrer"
            >GitHub Repo</a
          >
        </div>
      </header>

      <section class="section">
        <h2>Executive Summary</h2>
        <div class="grid">
          <div class="card">
            <h3>Challenge</h3>
            <p>
              Prompt changes and model updates introduced quality drift, safety
              risks, and inconsistent responses.
            </p>
          </div>
          <div class="card">
            <h3>Solution</h3>
            <p>
              Built evaluation pipelines with golden prompts, toxicity
              detection, and scoring dashboards.
            </p>
          </div>
          <div class="card">
            <h3>Impact</h3>
            <p>
              Created repeatable guardrails that surface regressions before
              release and inform model tuning.
            </p>
          </div>
        </div>
      </section>

      <section class="section">
        <h2>Coverage Map</h2>
        <div class="grid">
          <div class="card">
            <h3>Prompt Suites</h3>
            <p>Regression prompts across intents, personas, and tone.</p>
          </div>
          <div class="card">
            <h3>Safety &amp; Bias</h3>
            <p>Toxicity, bias, and policy compliance detection.</p>
          </div>
          <div class="card">
            <h3>Retrieval QA</h3>
            <p>Grounded response checks for RAG pipelines.</p>
          </div>
          <div class="card">
            <h3>Latency &amp; Cost</h3>
            <p>Token usage and response time monitoring.</p>
          </div>
        </div>
      </section>

      <section class="section">
        <h2>Automation Layers</h2>
        <div class="grid">
          <div class="card">
            <h3>Prompt Validation</h3>
            <p>Structured tests with expected response patterns.</p>
          </div>
          <div class="card">
            <h3>Heuristic Scoring</h3>
            <p>Quality metrics for relevance and completeness.</p>
          </div>
          <div class="card">
            <h3>Red Team Suite</h3>
            <p>Adversarial prompts for safety validation.</p>
          </div>
          <div class="card">
            <h3>Human Review</h3>
            <p>Spot-check workflows and feedback logging.</p>
          </div>
        </div>
      </section>

      <section class="section">
        <h2>Quality Signals Tracked</h2>
        <div class="grid">
          <div class="kpi">
            <span>Quality</span>
            <strong>Relevance, accuracy, and grounding scores.</strong>
          </div>
          <div class="kpi">
            <span>Safety</span>
            <strong>Toxicity/bias thresholds and policy violations.</strong>
          </div>
          <div class="kpi">
            <span>Latency</span>
            <strong>P95 response time and token spend.</strong>
          </div>
          <div class="kpi">
            <span>Drift</span>
            <strong>Diffs against baseline response sets.</strong>
          </div>
        </div>
      </section>

      <section class="section">
        <h2>CI/CD Pipeline Gates</h2>
        <div class="pipeline">
          <div class="pipeline-step">
            <span>Stage 1</span>
            Prompt linting + dataset validation.
          </div>
          <div class="pipeline-step">
            <span>Stage 2</span>
            Offline evals + scoring regression.
          </div>
          <div class="pipeline-step">
            <span>Stage 3</span>
            Red-team suite and safety checks.
          </div>
          <div class="pipeline-step">
            <span>Stage 4</span>
            Publish scorecards to QA dashboard.
          </div>
        </div>
      </section>

      <section class="section">
        <h2>Artifacts & Reporting</h2>
        <div class="flow">
          <span>Evaluation Report</span><span>Bias Heatmap</span
          ><span>Prompt Library</span><span>Safety Scorecard</span>
        </div>
      </section>

      <section class="section">
        <h2>Stack In Action</h2>
        <div class="grid">
          <div class="card">
            <h3>OpenAI API Harness</h3>
            <p>
              Regression prompts run with deterministic settings and logging.
            </p>
            <div class="code-block">
              from openai import OpenAI client = OpenAI() response =
              client.responses.create( model="gpt-4.1-mini", input="Summarize
              the checkout failure." ) print(response.output_text)
            </div>
          </div>
          <div class="card">
            <h3>pytest Evaluation Suite</h3>
            <p>Parameterized tests validate responses across prompt sets.</p>
            <div class="code-block">
              @pytest.mark.parametrize("case", cases) def
              test_grounded_answers(case, llm): result =
              llm.complete(case.prompt) assert case.expected_fact in
              result.lower()
            </div>
          </div>
          <div class="card">
            <h3>ML Scoring</h3>
            <p>Vector similarity scoring tracks response drift.</p>
            <div class="code-block">
              from sklearn.metrics.pairwise import cosine_similarity score =
              cosine_similarity(vec(result), vec(expected))[0][0] assert score >
              0.82
            </div>
          </div>
          <div class="card">
            <h3>Analytics Snapshot</h3>
            <p>Aggregate metrics for latency, cost, and quality.</p>
            <div class="code-block">
              import pandas as pd summary = df.groupby("suite")["score"].mean()
              print(summary.sort_values())
            </div>
          </div>
        </div>
      </section>

      <section class="section">
        <h2>Sample Eval Test</h2>
        <div class="code-block">
          def test_summary_quality(llm, eval_case): result =
          llm.complete(eval_case.prompt) assert "payment failed" in
          result.lower() assert score_relevance(result, eval_case.references)
          &gt; 0.82
        </div>
      </section>

      <section class="section">
        <h2>Validation Flow</h2>
        <div class="flow">
          <span>Load prompt set</span><span>Run evals</span
          ><span>Score + review</span><span>Approve release</span>
        </div>
      </section>

      <footer>
        Portfolio case study scaffold. Replace placeholders with live outputs
        when ready.
      </footer>
    </div>
  </body>
</html>
